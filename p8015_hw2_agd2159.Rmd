---
title: "P8015_Hw2_agd2159"
output: github_document
author: "Zander De Jesus"
date: "10-04-2023"
---

## Problem 1: FiveThirtyEight Data Cleaning

Begin by importing necessary libraries:
```{r}
library(tidyverse)
library(readxl)
```

**Task 1:** Reading in and Cleaning Monthly Political Datasheet

Separating Date into 3 separate columns.
Using a secondary month tibble dataframe in order to join the months from a numeric to a character name replacement. 
```{r}

months_df =
  tibble(
    month_num = 1:12,
    month_abb = month.abb,
    month = month.name
  )
months_df

pols_df = read_csv("538_Data/pols-month.csv") |> 
  janitor::clean_names() |> 
  separate(col = mon,
           into = c("year", "month_num", "day"),
           sep = "-", 
           convert = TRUE) |> 
  mutate(president = recode(prez_gop, "0" = "Dem", "1" = "Gop", "2" = "Gop")) |> 
  left_join(x = _, y = months_df) |> 
  select(year, month, everything(), -day, -starts_with("prez")) 

pols_df

```

**Task 2** Import and clean SNP Csv File
Main tidying goal is to reformat and separate the date item from one column list to separate year, month, day, characteristics. Will join with `months_df` as in previous task for pols_df.
```{r}
snp_df = read_csv("538_Data/snp.csv", col_types = cols(date = col_date(format = "%m/%d/%y")) ) |> 
  janitor::clean_names() |> 
  separate(date, into = c("year", "month_num", "day"), convert = TRUE) |>
  mutate(
    year = if_else(year > 2023, year - 100, year)) |> 
  left_join(x = _, y = months_df) |> 
  select(year, month, close) 

snp_df
```

**Task 3** Cleaning Unemployment File, Preparing for Join with two previously cleaned dataframes.
```{r}
unemployment_df = 
  read_csv("538_Data/unemployment.csv") |> 
  rename(year = Year) |> 
  pivot_longer(
    Jan:Dec,
    names_to = "month_abb",
    values_to = "unemployment"
  ) |> 
  left_join(x = _, y = months_df) |> 
  select(year, month, unemployment)
unemployment_df

```

**Task 4** Final join of all three datasets
```{r}
Df_538 = 
  left_join(pols_df, snp_df) |> 
  left_join(x = _, y = unemployment_df)

str(Df_538)

```



## Problem 2: Mr. Trashwheel Dataset
The Trashwheel Dataset contains 6 separate sheets across the excel file, including pages for notes and discussing methodologies. Since this problem requires only 3 of the 4 trashwheel machines for data compilation, each trashwheel can be read in as a individualized tibble dataframe using the `sheet` and `range` arguments of `read_excel`. Calculations will then be further done on these dataframes before joining. 

Sum rows at the bottom of each table made in excel are excluded so that they are not mistaken as values in upcoming calculations. 

```{r Importing Initial 3 Trash Datasets }

mrtrashwheel = read_excel("Trashwheel_Data/202309 Trash Wheel Collection Data.xlsx",
                             sheet = 1, range = "A2:N586")

professortrash = read_excel("Trashwheel_Data/202309 Trash Wheel Collection Data.xlsx",
                             sheet = 2, range = "A2:M108")

gwyndatrash = read_excel("Trashwheel_Data/202309 Trash Wheel Collection Data.xlsx",
                             sheet = 4, range = "A2:L158")


```
Professortrash and Gwynda do not contain a variable column counting the "Sports Ball" category of trash collected, as Mr Trashwheel does. Gwynda also lacks a column on Glass Bottles collected, and a significant portion of Wrappers column is "NA." Since MrTrashwheel has both the largest amount of data and the most thorough reporting across its columns, I believe it will be best to clean the data to fit a lot of the standards that MrTrashwheel provides before joining. 

**Task 1**

Starting by cleaning `mrtrashwheel` dataframe. Adding Identifying Trashwheel Column that can serve as a joining key ahead of `bind_rows()` function.

The note on Homes Powered state that each ton in trash week produces an avg. 500 kilowatts of power. To standardize this across all rows, weight variable will be multiplied by 500 and divided by the average home energy expenditure 30 kilowatts/day to get the average homes powered by each trashload.
For the calculation, Homes Powered and Weight were renamed to avoid the parentheses and asterisks from interfering with the calculation being misread as operators.

```{r mrtrashwheel tidy}

mrtrashwheel = mrtrashwheel |> 
  mutate(WheelID=c("MrTrashwheel"),.before=Dumpster) |> 
  rename("Homes_Powered" = "Homes Powered*", 
         "Wt_tons" = "Weight (tons)",
         "Volume_yd3" = "Volume (cubic yards)") |> 
  transform(Year = as.numeric(Year)) |> 
  mutate(Homes_Powered = ((Wt_tons*500)/30))

```

**Task 2**

These tidying functions are replicated on the Professor and Gwynda datasets, which both already had the Year Column as a numeric value. 

```{r remaining 2 tidy}

professortrash = professortrash |> 
  mutate(WheelID=c("ProfTrashwheel"),.before=Dumpster) |> 
  rename("Homes_Powered" = "Homes Powered*", "Wt_tons" = "Weight (tons)") |> 
  mutate(Homes_Powered = ((Wt_tons*500)/30))

gwyndatrash = gwyndatrash |> 
  mutate(WheelID=c("GwyndaTrashwheel"),.before=Dumpster) |> 
  rename("Homes_Powered" = "Homes Powered*", "Wt_tons" = "Weight (tons)") |> 
  mutate(Homes_Powered = ((Wt_tons*500)/30))

```

**Task 3**

Ahead of binding, all three dataframes given cleaned snakecase names to standardize a few discontinuities where words are separated with periods and spaces across the three sheets. (Ex: Glass Bottles vs. Glass.Bottles)

Binding the three cleaned dataframes together and overall stats summary. 

```{r Binding}
mrtrashwheel = mrtrashwheel |> 
  janitor::clean_names()

professortrash = professortrash |> 
  janitor::clean_names()

gwyndatrash = gwyndatrash |> 
  janitor::clean_names()

TotalTrashwheel_tidy =
  bind_rows(mrtrashwheel, professortrash, gwyndatrash)

```

"a similar process to import, clean, and organize the data for Professor Trash Wheel and Gwynnda, and combine these with the Mr. Trash Wheel dataset to produce a single tidy dataset. To keep track of which Trash Wheel is which, you may need to add an additional variable to all datasets before combining." 


## Problem 3: 




