---
title: "P8015_Hw2_agd2159"
output: github_document
author: "Zander De Jesus"
date: "10-04-2023"
---

## Problem 1: FiveThirtyEight Data Cleaning

Begin by importing necessary libraries:
```{r}
library(tidyverse)
library(readxl)
```

**Task 1:** Reading in and Cleaning Monthly Political Datasheet

Separating Date into 3 separate columns.
Using a secondary month tibble dataframe in order to join the months from a numeric to a character name replacement. 
```{r}

months_df =
  tibble(
    month_num = 1:12,
    month_abb = month.abb,
    month = month.name
  )
months_df

pols_df = read_csv("538_Data/pols-month.csv") |> 
  janitor::clean_names() |> 
  separate(col = mon,
           into = c("year", "month_num", "day"),
           sep = "-", 
           convert = TRUE) |> 
  mutate(president = recode(prez_gop, "0" = "Dem", "1" = "Gop", "2" = "Gop")) |> 
  left_join(x = _, y = months_df) |> 
  select(year, month, everything(), -day, -starts_with("prez")) 

pols_df

```

**Task 2** Import and clean SNP Csv File
Main tidying goal is to reformat and separate the date item from one column list to separate year, month, day, characteristics. Will join with `months_df` as in previous task for pols_df.
```{r}
snp_df = read_csv("538_Data/snp.csv", col_types = cols(date = col_date(format = "%m/%d/%y")) ) |> 
  janitor::clean_names() |> 
  separate(date, into = c("year", "month_num", "day"), convert = TRUE) |>
  mutate(
    year = if_else(year > 2023, year - 100, year)) |> 
  left_join(x = _, y = months_df) |> 
  select(year, month, close) 

snp_df
```

**Task 3** Cleaning Unemployment File, Preparing for Join with two previously cleaned dataframes.
```{r}
unemployment_df = 
  read_csv("538_Data/unemployment.csv") |> 
  rename(year = Year) |> 
  pivot_longer(
    Jan:Dec,
    names_to = "month_abb",
    values_to = "unemployment"
  ) |> 
  left_join(x = _, y = months_df) |> 
  select(year, month, unemployment)
unemployment_df

```

**Task 4** Final join of all three datasets
```{r}
Df_538 = 
  left_join(pols_df, snp_df) |> 
  left_join(x = _, y = unemployment_df)

str(Df_538)

```



## Problem 2: Mr. Trashwheel Dataset
The Trashwheel Dataset contains 6 separate sheets across the excel file, including pages for notes and discussing methodologies. Since this problem requires only 3 of the 4 trashwheel machines for data compilation, each trashwheel can be read in as a individualized tibble dataframe using the `sheet` and `range` arguments of `read_excel`. Calculations will then be further done on these dataframes before joining. 

Sum rows at the bottom of each table made in excel are excluded so that they are not mistaken as values in upcoming calculations. 

```{r Importing Initial 3 Trash Datasets }

mrtrashwheel = read_excel("Trashwheel_Data/202309 Trash Wheel Collection Data.xlsx",
                             sheet = 1, range = "A2:N586")

professortrash = read_excel("Trashwheel_Data/202309 Trash Wheel Collection Data.xlsx",
                             sheet = 2, range = "A2:M108")

gwyndatrash = read_excel("Trashwheel_Data/202309 Trash Wheel Collection Data.xlsx",
                             sheet = 4, range = "A2:L158")


```
Professortrash and Gwynda do not contain a variable column counting the "Sports Ball" category of trash collected, as Mr Trashwheel does. Gwynda also lacks a column on Glass Bottles collected, and a significant portion of Wrappers column is "NA." Since MrTrashwheel has both the largest amount of data and the most thorough reporting across its columns, I believe it will be best to clean the data to fit a lot of the standards that MrTrashwheel provides before joining. 

**Task 1**

Starting by cleaning `mrtrashwheel` dataframe. Adding Identifying Trashwheel Column that can serve as a joining key ahead of `bind_rows()` function.

The note on Homes Powered state that each ton in trash week produces an avg. 500 kilowatts of power. To standardize this across all rows, weight variable will be multiplied by 500 and divided by the average home energy expenditure 30 kilowatts/day to get the average homes powered by each trashload.
For the calculation, Homes Powered and Weight were renamed to avoid the parentheses and asterisks from interfering with the calculation being misread as operators.

```{r mrtrashwheel tidy}

mrtrashwheel = mrtrashwheel |> 
  mutate(WheelID=c("MrTrashwheel"),.before=Dumpster) |> 
  rename("Homes_Powered" = "Homes Powered*", "Wt_tons" = "Weight (tons)") |> 
  transform(Year = as.numeric(Year)) |> 
  mutate(Homes_Powered = ((Wt_tons*500)/30))

```

**Task 2**

These tidying functions are replicated on the Professor and Gwynda datasets, which both already had the Year Column as a numeric value. 

```{r remaining 2 tidy}

professortrash = professortrash |> 
  mutate(WheelID=c("ProfTrashwheel"),.before=Dumpster) |> 
  rename("Homes_Powered" = "Homes Powered*", "Wt_tons" = "Weight (tons)") |> 
  mutate(Homes_Powered = ((Wt_tons*500)/30))

gwyndatrash = gwyndatrash |> 
  mutate(WheelID=c("GwyndaTrashwheel"),.before=Dumpster) |> 
  rename("Homes_Powered" = "Homes Powered*", "Wt_tons" = "Weight (tons)") |> 
  mutate(Homes_Powered = ((Wt_tons*500)/30))

```

**Task 3**

Ahead of binding, all three dataframes given cleaned snakecase names to standardize a few discontinuities where words are separated with periods and spaces across the three sheets. (Ex: Glass Bottles vs. Glass.Bottles)

Binding the three cleaned dataframes together and overall stats summary. 

```{r Binding}
mrtrashwheel = mrtrashwheel |> 
  janitor::clean_names()

professortrash = professortrash |> 
  janitor::clean_names()

gwyndatrash = gwyndatrash |> 
  janitor::clean_names()

TotalTrashwheel_tidy =
  bind_rows(mrtrashwheel, professortrash, gwyndatrash)

summary(TotalTrashwheel_tidy)

#Testing this to see how best to call inline R code when there are a few NA's
sum(pull(TotalTrashwheel_tidy, "wt_tons"), na.rm = TRUE)

```

**Data Summary**

The `TotalTrashwheel_tidy` compiled dataframe has `r nrow(TotalTrashwheel_tidy)` observations and `r ncol(TotalTrashwheel_tidy)` variables across three different trashwheel collection programs. A total of **`r sum(pull(TotalTrashwheel_tidy, "wt_tons"), na.rm = TRUE)`** tons of trash was collected from all three of these trashwheels combined. This much trash weight represents about **`r sum(pull(TotalTrashwheel_tidy, "homes_powered"), na.rm = TRUE)`** homes that were powered by this trash. When measured in volume, an average of  **`r mean(pull(TotalTrashwheel_tidy, "volume_cubic_yards"), na.rm = TRUE)`** cubic yards of trash was collected when calculating across all trashwheels' datasets. 


Requested in Problem 2: Professor Trashwheel collected a total of  **`r sum(pull(professortrash, "wt_tons"), na.rm = TRUE)` tons** of trash across its activity period (2017-2023). Average of **`r mean(pull(professortrash, "wt_tons"), na.rm = TRUE)` tons** daily in this study period.

Similarly, Mr. Trashwheel collected a total of  **`r sum(pull(mrtrashwheel, "wt_tons"), na.rm = TRUE)` tons** of trash from 2014 - 2023. Average of **`r mean(pull(mrtrashwheel, "wt_tons"), na.rm = TRUE)` tons** daily across this period. Since it is much older, it has a larger overall tonnage collected, and a slightly higher average daily trash collection than Professor and Gwynda.

Gwynda Trashwheel collected a total of  **`r sum(pull(gwyndatrash, "wt_tons"), na.rm = TRUE)` tons** of trash from 2021 - 2023. Average of **`r mean(pull(gwyndatrash, "wt_tons"), na.rm = TRUE)` tons** daily across this period.

```{r}
GwyndaCig = gwyndatrash |> 
  filter(month == "July", year == 2021)
sum(pull(GwyndaCig, "cigarette_butts"), na.rm = TRUE)
```

Gwynda Trashwheel collected a total of **16300** cigarette butts from the recorded days within July 2021.


## Problem 3: MCI Baseline and Amyloid Analysis

Beginning by reading in both the Baseline and Amyloid participant datasets to monitor Mild Cognitive Impairment and the onset of Alzheimer's Disease biomarkers. First row skipped due to having notes / table description.

```{r Initial CSV Imports}

mci_baseline = read_csv("MCI_Data/MCI_baseline.csv", skip = 1) |> 
  janitor::clean_names()

mci_amyloid = read_csv("MCI_Data/mci_amyloid.csv", skip = 1) |> 
  janitor::clean_names()

```

Initial column specification information using the `spec()` function indicates that a number of the column variables are coded incorrectly as either characters or numerics, and need to be swapped. First portion of coding will be restandardizing these labels to correct factors across both datasets to prepare for merger.

**Task 1**

Baseline CSV Cleaning Below, with specific function at end to filter out participants that had MCI at Baseline:
```{r cleaning baseline}

baseline_tidy = mci_baseline |> 
  rename("study_id" = "id") |>
  mutate(sex = case_match(
    sex,
    1 ~ "male",
    0 ~ "female"),
    sex = as.factor(sex)) |> 
  mutate(apoe4 = case_match(
    apoe4,
    1 ~ "carrier",
    0 ~ "non-carrier"),
    apoe4 = as.factor(apoe4)) |> 
  filter(!(age_at_onset <= current_age) | age_at_onset == ".")

```

```{r Operation for P3 Data Summary Questions, echo=FALSE}
Women_subset = filter(baseline_tidy, sex == "female")

```

**MCI Baseline Data Summary**

The `baseline_tidy` dataframe contains `r nrow(baseline_tidy)` recruited participants, and classifies `r ncol(baseline_tidy)` variables. It appears that **4 participants of the original 483 had MCI at baseline** and need to be excluded from the stats analysis to avoid bias.

The average age of participants at baseline was **`r mean(pull(baseline_tidy, current_age))`** years old. Of these baseline participants, **93** develop MCI during the study based on `age_at_onset` data recorded.

**`r nrow(Women_subset)`** of the baseline participants were women, and **63** women participants were APOE4 Carriers. 63/210 = 30% of women, 63/479 = 13% of total cohort.


**Task 2** 

The `mci_amyloid` dataframe is already quite clean for a dataset after standardizing the column headings with `clean_names()`. This dataframe has more participants based on id than the mci_baseline dataframe, going up to 495 while baseline only goes up to 483 before MCI at baseline is excluded.

Manipulating Amyloid Biomarker Time Series Dataframe. The longtitudinal time series of biomarker values could be reoriented using `pivot_longer` and may be helpful for sorting within patient changes in amyloid biomarker. 

```{r}

amyloid_tidy = mci_amyloid |> 
  pivot_longer(
    baseline:time_8,
    names_to = "visit_years",
    values_to = "amyloid_ratio") |> 
  transform(amyloid_ratio = as.numeric(amyloid_ratio))

```

Joining baseline with amyloid datasets together. I decided to provide mci_joined versions that have amyloid dataframe both in the original wide format and in the manipulated pivot_longer results form.

```{r}
mci_joined_long = left_join(baseline_tidy, amyloid_tidy, by = "study_id")

mci_joined_wide = left_join(baseline_tidy, mci_amyloid, by = "study_id")
```

After looking at both versions of the mci joined dataframe, I decided that the joined table containing the original wide form amyloid data, **`mc_joined_wide`** should be considered the final table version. This is because when amyloid data is transformed with pivot_longer, there is too much repetition in baseline data and follow-up ages are not recorded. 

mci_joined_wide will be saved as the final csv file within the github repository.
This file only includes participants found in both dataframes.

```{r}
mci_joined_final = mci_joined_wide

write_csv(mci_joined_final,"MCI_Data/mci_joined_final.csv")

```

**mci_joined_final.csv** is the export product that can be found within **MCI_Data subfolder.**


