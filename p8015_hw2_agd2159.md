P8015_Hw2_agd2159
================
Zander De Jesus
10-04-2023

## Problem 1: FiveThirtyEight Data Cleaning

Begin by importing necessary libraries:

``` r
library(tidyverse)
```

    ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
    ## ✔ dplyr     1.1.3     ✔ readr     2.1.4
    ## ✔ forcats   1.0.0     ✔ stringr   1.5.0
    ## ✔ ggplot2   3.4.3     ✔ tibble    3.2.1
    ## ✔ lubridate 1.9.2     ✔ tidyr     1.3.0
    ## ✔ purrr     1.0.2     
    ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
    ## ✖ dplyr::filter() masks stats::filter()
    ## ✖ dplyr::lag()    masks stats::lag()
    ## ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors

``` r
library(readxl)
```

**Task 1:** Reading in and Cleaning Monthly Political Datasheet

Separating Date into 3 separate columns. Using a secondary month tibble
dataframe in order to join the months from a numeric to a character name
replacement.

``` r
months_df =
  tibble(
    month_num = 1:12,
    month_abb = month.abb,
    month = month.name
  )
months_df
```

    ## # A tibble: 12 × 3
    ##    month_num month_abb month    
    ##        <int> <chr>     <chr>    
    ##  1         1 Jan       January  
    ##  2         2 Feb       February 
    ##  3         3 Mar       March    
    ##  4         4 Apr       April    
    ##  5         5 May       May      
    ##  6         6 Jun       June     
    ##  7         7 Jul       July     
    ##  8         8 Aug       August   
    ##  9         9 Sep       September
    ## 10        10 Oct       October  
    ## 11        11 Nov       November 
    ## 12        12 Dec       December

``` r
pols_df = read_csv("538_Data/pols-month.csv") |> 
  janitor::clean_names() |> 
  separate(col = mon,
           into = c("year", "month_num", "day"),
           sep = "-", 
           convert = TRUE) |> 
  mutate(president = recode(prez_gop, "0" = "Dem", "1" = "Gop", "2" = "Gop")) |> 
  left_join(x = _, y = months_df) |> 
  select(year, month, everything(), -day, -starts_with("prez")) 
```

    ## Rows: 822 Columns: 9
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## dbl  (8): prez_gop, gov_gop, sen_gop, rep_gop, prez_dem, gov_dem, sen_dem, r...
    ## date (1): mon
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.
    ## Joining with `by = join_by(month_num)`

``` r
pols_df
```

    ## # A tibble: 822 × 11
    ##     year month     month_num gov_gop sen_gop rep_gop gov_dem sen_dem rep_dem
    ##    <int> <chr>         <int>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>
    ##  1  1947 January           1      23      51     253      23      45     198
    ##  2  1947 February          2      23      51     253      23      45     198
    ##  3  1947 March             3      23      51     253      23      45     198
    ##  4  1947 April             4      23      51     253      23      45     198
    ##  5  1947 May               5      23      51     253      23      45     198
    ##  6  1947 June              6      23      51     253      23      45     198
    ##  7  1947 July              7      23      51     253      23      45     198
    ##  8  1947 August            8      23      51     253      23      45     198
    ##  9  1947 September         9      23      51     253      23      45     198
    ## 10  1947 October          10      23      51     253      23      45     198
    ## # ℹ 812 more rows
    ## # ℹ 2 more variables: president <chr>, month_abb <chr>

**Task 2** Import and clean SNP Csv File Main tidying goal is to
reformat and separate the date item from one column list to separate
year, month, day, characteristics. Will join with `months_df` as in
previous task for pols_df.

``` r
snp_df = read_csv("538_Data/snp.csv", col_types = cols(date = col_date(format = "%m/%d/%y")) ) |> 
  janitor::clean_names() |> 
  separate(date, into = c("year", "month_num", "day"), convert = TRUE) |>
  mutate(
    year = if_else(year > 2023, year - 100, year)) |> 
  left_join(x = _, y = months_df) |> 
  select(year, month, close) 
```

    ## Joining with `by = join_by(month_num)`

``` r
snp_df
```

    ## # A tibble: 787 × 3
    ##     year month    close
    ##    <dbl> <chr>    <dbl>
    ##  1  2015 July     2080.
    ##  2  2015 June     2063.
    ##  3  2015 May      2107.
    ##  4  2015 April    2086.
    ##  5  2015 March    2068.
    ##  6  2015 February 2104.
    ##  7  2015 January  1995.
    ##  8  2014 December 2059.
    ##  9  2014 November 2068.
    ## 10  2014 October  2018.
    ## # ℹ 777 more rows

**Task 3** Cleaning Unemployment File, Preparing for Join with two
previously cleaned dataframes.

``` r
unemployment_df = 
  read_csv("538_Data/unemployment.csv") |> 
  rename(year = Year) |> 
  pivot_longer(
    Jan:Dec,
    names_to = "month_abb",
    values_to = "unemployment"
  ) |> 
  left_join(x = _, y = months_df) |> 
  select(year, month, unemployment)
```

    ## Rows: 68 Columns: 13
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## dbl (13): Year, Jan, Feb, Mar, Apr, May, Jun, Jul, Aug, Sep, Oct, Nov, Dec
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.
    ## Joining with `by = join_by(month_abb)`

``` r
unemployment_df
```

    ## # A tibble: 816 × 3
    ##     year month     unemployment
    ##    <dbl> <chr>            <dbl>
    ##  1  1948 January            3.4
    ##  2  1948 February           3.8
    ##  3  1948 March              4  
    ##  4  1948 April              3.9
    ##  5  1948 May                3.5
    ##  6  1948 June               3.6
    ##  7  1948 July               3.6
    ##  8  1948 August             3.9
    ##  9  1948 September          3.8
    ## 10  1948 October            3.7
    ## # ℹ 806 more rows

**Task 4** Final join of all three datasets

``` r
Df_538 = 
  left_join(pols_df, snp_df) |> 
  left_join(x = _, y = unemployment_df)
```

    ## Joining with `by = join_by(year, month)`
    ## Joining with `by = join_by(year, month)`

``` r
str(Df_538)
```

    ## tibble [822 × 13] (S3: tbl_df/tbl/data.frame)
    ##  $ year        : num [1:822] 1947 1947 1947 1947 1947 ...
    ##  $ month       : chr [1:822] "January" "February" "March" "April" ...
    ##  $ month_num   : int [1:822] 1 2 3 4 5 6 7 8 9 10 ...
    ##  $ gov_gop     : num [1:822] 23 23 23 23 23 23 23 23 23 23 ...
    ##  $ sen_gop     : num [1:822] 51 51 51 51 51 51 51 51 51 51 ...
    ##  $ rep_gop     : num [1:822] 253 253 253 253 253 253 253 253 253 253 ...
    ##  $ gov_dem     : num [1:822] 23 23 23 23 23 23 23 23 23 23 ...
    ##  $ sen_dem     : num [1:822] 45 45 45 45 45 45 45 45 45 45 ...
    ##  $ rep_dem     : num [1:822] 198 198 198 198 198 198 198 198 198 198 ...
    ##  $ president   : chr [1:822] "Dem" "Dem" "Dem" "Dem" ...
    ##  $ month_abb   : chr [1:822] "Jan" "Feb" "Mar" "Apr" ...
    ##  $ close       : num [1:822] NA NA NA NA NA NA NA NA NA NA ...
    ##  $ unemployment: num [1:822] NA NA NA NA NA NA NA NA NA NA ...

## Problem 2: Mr. Trashwheel Dataset

The Trashwheel Dataset contains 6 separate sheets across the excel file,
including pages for notes and discussing methodologies. Since this
problem requires only 3 of the 4 trashwheel machines for data
compilation, each trashwheel can be read in as a individualized tibble
dataframe using the `sheet` and `range` arguments of `read_excel`.
Calculations will then be further done on these dataframes before
joining.

Sum rows at the bottom of each table made in excel are excluded so that
they are not mistaken as values in upcoming calculations.

``` r
mrtrashwheel = read_excel("Trashwheel_Data/202309 Trash Wheel Collection Data.xlsx",
                             sheet = 1, range = "A2:N586")

professortrash = read_excel("Trashwheel_Data/202309 Trash Wheel Collection Data.xlsx",
                             sheet = 2, range = "A2:M108")

gwyndatrash = read_excel("Trashwheel_Data/202309 Trash Wheel Collection Data.xlsx",
                             sheet = 4, range = "A2:L158")
```

Professortrash and Gwynda do not contain a variable column counting the
“Sports Ball” category of trash collected, as Mr Trashwheel does. Gwynda
also lacks a column on Glass Bottles collected, and a significant
portion of Wrappers column is “NA.” Since MrTrashwheel has both the
largest amount of data and the most thorough reporting across its
columns, I believe it will be best to clean the data to fit a lot of the
standards that MrTrashwheel provides before joining.

**Task 1**

Starting by cleaning `mrtrashwheel` dataframe. Adding Identifying
Trashwheel Column that can serve as a joining key ahead of `bind_rows()`
function.

The note on Homes Powered state that each ton in trash week produces an
avg. 500 kilowatts of power. To standardize this across all rows, weight
variable will be multiplied by 500 and divided by the average home
energy expenditure 30 kilowatts/day to get the average homes powered by
each trashload. For the calculation, Homes Powered and Weight were
renamed to avoid the parentheses and asterisks from interfering with the
calculation being misread as operators.

``` r
mrtrashwheel = mrtrashwheel |> 
  mutate(WheelID=c("MrTrashwheel"),.before=Dumpster) |> 
  rename("Homes_Powered" = "Homes Powered*", "Wt_tons" = "Weight (tons)") |> 
  transform(Year = as.numeric(Year)) |> 
  mutate(Homes_Powered = ((Wt_tons*500)/30))
```

**Task 2**

These tidying functions are replicated on the Professor and Gwynda
datasets, which both already had the Year Column as a numeric value.

``` r
professortrash = professortrash |> 
  mutate(WheelID=c("ProfTrashwheel"),.before=Dumpster) |> 
  rename("Homes_Powered" = "Homes Powered*", "Wt_tons" = "Weight (tons)") |> 
  mutate(Homes_Powered = ((Wt_tons*500)/30))

gwyndatrash = gwyndatrash |> 
  mutate(WheelID=c("GwyndaTrashwheel"),.before=Dumpster) |> 
  rename("Homes_Powered" = "Homes Powered*", "Wt_tons" = "Weight (tons)") |> 
  mutate(Homes_Powered = ((Wt_tons*500)/30))
```

**Task 3**

Ahead of binding, all three dataframes given cleaned snakecase names to
standardize a few discontinuities where words are separated with periods
and spaces across the three sheets. (Ex: Glass Bottles
vs. Glass.Bottles)

Binding the three cleaned dataframes together and overall stats summary.

``` r
mrtrashwheel = mrtrashwheel |> 
  janitor::clean_names()

professortrash = professortrash |> 
  janitor::clean_names()

gwyndatrash = gwyndatrash |> 
  janitor::clean_names()

TotalTrashwheel_tidy =
  bind_rows(mrtrashwheel, professortrash, gwyndatrash)

summary(TotalTrashwheel_tidy)
```

    ##    wheel_id            dumpster      month                year     
    ##  Length:846         Min.   :  1   Length:846         Min.   :2014  
    ##  Class :character   1st Qu.: 71   Class :character   1st Qu.:2017  
    ##  Mode  :character   Median :162   Mode  :character   Median :2019  
    ##                     Mean   :223                      Mean   :2019  
    ##                     3rd Qu.:373                      3rd Qu.:2021  
    ##                     Max.   :584                      Max.   :2023  
    ##                     NA's   :1                        NA's   :1     
    ##       date                           wt_tons      volume_cubic_yards
    ##  Min.   :1900-01-20 00:00:00.00   Min.   :0.610   Min.   : 5.00     
    ##  1st Qu.:2017-06-21 00:00:00.00   1st Qu.:2.490   1st Qu.:15.00     
    ##  Median :2019-10-25 00:00:00.00   Median :3.070   Median :15.00     
    ##  Mean   :2019-06-08 04:53:06.75   Mean   :3.009   Mean   :15.13     
    ##  3rd Qu.:2021-11-04 00:00:00.00   3rd Qu.:3.540   3rd Qu.:15.00     
    ##  Max.   :2023-06-30 00:00:00.00   Max.   :5.620   Max.   :20.00     
    ##  NA's   :1                        NA's   :1       NA's   :1         
    ##  plastic_bottles  polystyrene    cigarette_butts  glass_bottles   
    ##  Min.   :   0    Min.   :    0   Min.   :     0   Min.   :  0.00  
    ##  1st Qu.:1000    1st Qu.:  280   1st Qu.:  3200   1st Qu.: 10.00  
    ##  Median :1980    Median :  950   Median :  5500   Median : 18.00  
    ##  Mean   :2296    Mean   : 1631   Mean   : 15592   Mean   : 20.89  
    ##  3rd Qu.:2900    3rd Qu.: 2400   3rd Qu.: 16000   3rd Qu.: 28.00  
    ##  Max.   :9830    Max.   :11528   Max.   :310000   Max.   :110.00  
    ##  NA's   :2       NA's   :2       NA's   :2        NA's   :157     
    ##   plastic_bags      wrappers      sports_balls   homes_powered  
    ##  Min.   :    0   Min.   :  180   Min.   : 0.00   Min.   :10.17  
    ##  1st Qu.:  280   1st Qu.:  840   1st Qu.: 6.00   1st Qu.:41.50  
    ##  Median :  680   Median : 1380   Median :11.00   Median :51.17  
    ##  Mean   : 1082   Mean   : 2330   Mean   :13.17   Mean   :50.16  
    ##  3rd Qu.: 1400   3rd Qu.: 2635   3rd Qu.:18.25   3rd Qu.:59.00  
    ##  Max.   :13450   Max.   :20100   Max.   :56.00   Max.   :93.67  
    ##  NA's   :2       NA's   :119     NA's   :262     NA's   :1

``` r
#Testing this to see how best to call inline R code when there are a few NA's
sum(pull(TotalTrashwheel_tidy, "wt_tons"), na.rm = TRUE)
```

    ## [1] 2543.01

**Data Summary**

The `TotalTrashwheel_tidy` compiled dataframe has 846 observations and
15 variables across three different trashwheel collection programs. A
total of **2543.01** tons of trash was collected from all three of these
trashwheels combined. This much trash weight represents about
**4.23835^{4}** homes that were powered by this trash. When measured in
volume, an average of **15.1349112** cubic yards of trash was collected
when calculating across all trashwheels’ datasets.

Requested in Problem 2: Professor Trashwheel collected a total of
**216.26 tons** of trash across its activity period (2017-2023). Average
of **2.0401887 tons** daily in this study period.

Similarly, Mr. Trashwheel collected a total of **1875.1 tons** of trash
from 2014 - 2023. Average of **3.2107877 tons** daily across this
period. Since it is much older, it has a larger overall tonnage
collected, and a slightly higher average daily trash collection than
Professor and Gwynda.

Gwynda Trashwheel collected a total of **451.65 tons** of trash from
2021 - 2023. Average of **2.913871 tons** daily across this period.

``` r
GwyndaCig = gwyndatrash |> 
  filter(month == "July", year == 2021)
sum(pull(GwyndaCig, "cigarette_butts"), na.rm = TRUE)
```

    ## [1] 16300

Gwynda Trashwheel collected a total of **16300** cigarette butts from
the recorded days within July 2021.

## Problem 3: MCI Baseline and Amyloid Analysis

Beginning by reading in both the Baseline and Amyloid participant
datasets to monitor Mild Cognitive Impairment and the onset of
Alzheimer’s Disease biomarkers. First row skipped due to having notes /
table description.

``` r
mci_baseline = read_csv("MCI_Data/MCI_baseline.csv", skip = 1) |> 
  janitor::clean_names()
```

    ## Rows: 483 Columns: 6
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (1): Age at onset
    ## dbl (5): ID, Current Age, Sex, Education, apoe4
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
mci_amyloid = read_csv("MCI_Data/mci_amyloid.csv", skip = 1) |> 
  janitor::clean_names()
```

    ## Rows: 487 Columns: 6
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (5): Baseline, Time 2, Time 4, Time 6, Time 8
    ## dbl (1): Study ID
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

Initial column specification information using the `spec()` function
indicates that a number of the column variables are coded incorrectly as
either characters or numerics, and need to be swapped. First portion of
coding will be restandardizing these labels to correct factors across
both datasets to prepare for merger.

**Task 1**

Baseline CSV Cleaning Below, with specific function at end to filter out
participants that had MCI at Baseline:

``` r
baseline_tidy = mci_baseline |> 
  rename("study_id" = "id") |>
  mutate(sex = case_match(
    sex,
    1 ~ "male",
    0 ~ "female"),
    sex = as.factor(sex)) |> 
  mutate(apoe4 = case_match(
    apoe4,
    1 ~ "carrier",
    0 ~ "non-carrier"),
    apoe4 = as.factor(apoe4)) |> 
  filter(!(age_at_onset <= current_age) | age_at_onset == ".")
```

**MCI Baseline Data Summary**

The `baseline_tidy` dataframe contains 479 recruited participants, and
classifies 6 variables. It appears that **4 participants of the original
483 had MCI at baseline** and need to be excluded from the stats
analysis to avoid bias.

The average age of participants at baseline was **65.0286013** years
old. Of these baseline participants, **93** develop MCI during the study
based on `age_at_onset` data recorded.

**210** of the baseline participants were women, and **63** women
participants were APOE4 Carriers. 63/210 = 30% of women, 63/479 = 13% of
total cohort.

**Task 2**

The `mci_amyloid` dataframe is already quite clean for a dataset after
standardizing the column headings with `clean_names()`. This dataframe
has more participants based on id than the mci_baseline dataframe, going
up to 495 while baseline only goes up to 483 before MCI at baseline is
excluded.

Manipulating Amyloid Biomarker Time Series Dataframe. The longtitudinal
time series of biomarker values could be reoriented using `pivot_longer`
and may be helpful for sorting within patient changes in amyloid
biomarker.

``` r
amyloid_tidy = mci_amyloid |> 
  pivot_longer(
    baseline:time_8,
    names_to = "visit_years",
    values_to = "amyloid_ratio") |> 
  transform(amyloid_ratio = as.numeric(amyloid_ratio))
```

    ## Warning in eval(substitute(list(...)), `_data`, parent.frame()): NAs introduced
    ## by coercion

Joining baseline with amyloid datasets together. I decided to provide
mci_joined versions that have amyloid dataframe both in the original
wide format and in the manipulated pivot_longer results form.

``` r
mci_joined_long = left_join(baseline_tidy, amyloid_tidy, by = "study_id")

mci_joined_wide = left_join(baseline_tidy, mci_amyloid, by = "study_id")
```

After looking at both versions of the mci joined dataframe, I decided
that the joined table containing the original wide form amyloid data,
**`mc_joined_wide`** should be considered the final table version. This
is because when amyloid data is transformed with pivot_longer, there is
too much repetition in baseline data and follow-up ages are not
recorded.

mci_joined_wide will be saved as the final csv file within the github
repository.

``` r
mci_joined_final = mci_joined_wide

write_csv(mci_joined_final,"MCI_Data/mci_joined_final.csv")
```

**mci_joined_final.csv** is the export product that can be found within
**MCI_Data subfolder.**
